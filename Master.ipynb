{"cells":[{"cell_type":"markdown","metadata":{"id":"_XEEX8DT6jO9"},"source":["## Master Proposal\n","- Stacking several fine-tuned models to predict individually\n","    - distilbert\n","    - distilroberta\n","    - distilroberta + augmentation\n","- Add two linear classifiers on the top"]},{"cell_type":"markdown","metadata":{"id":"Y8bNCJ8dDY-l"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4C4FdL3C4ErW"},"outputs":[],"source":["import os, sys\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/NYCU NLP Final/')\n","sys.path.append('/content/drive/MyDrive/NYCU NLP Final/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zBLNYZK0DaqO","scrolled":false},"outputs":[],"source":["!pip install transformers datasets > /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHDI6cYj6a0J"},"outputs":[],"source":["from typing import Tuple, List, Dict, Union, Callable, Optional\n","from collections import OrderedDict\n","\n","import numpy as np\n","import pandas as pd\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vr52razv38Am"},"outputs":[],"source":["# parameters\n","SEED = 42\n","\n","# model\n","BASE_MODEL_CKPTS=[\n","    'models/both_distilbert',\n","    'models/both_distilroberta',\n","    'models/both_aug_distilroberta',\n","]\n","\n","EPOCHS=5\n","TRAIN_BATCH_SIZE=16\n","VALID_BATCH_SIZE=64\n","LEARNING_RATE=1e-4\n","WARMUP=400\n","\n","MODEL_SAVE_DIR = '0610_master_test'\n","# CHECKPOINT = 'checkpoint-4884'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7YtkZFM5AeR1"},"outputs":[],"source":["import random\n","\n","def set_seed():\n","    random.seed(SEED)\n","    np.random.seed(SEED)\n","    torch.manual_seed(SEED)\n","\n","set_seed()"]},{"cell_type":"markdown","metadata":{"id":"WiPavzo838Ao"},"source":["## Read Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uijxITXR38Ap"},"outputs":[],"source":["traindf = pd.read_csv('data/new_train.csv')\n","validdf = pd.read_csv('data/new_valid.csv')\n","testdf = pd.read_csv('data/new_test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tqj0GekSEi-z","scrolled":true},"outputs":[],"source":["print(f'# train: {len(traindf)}')\n","print(f'# valid: {len(validdf)}')\n","print(f'# test: {len(testdf)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHzktRKB38Ap"},"outputs":[],"source":["classes = traindf['label'].unique()\n","n_labels = len(classes)\n","\n","sent_id = {\n","    'sad':      0,  'trusting':     1,  'terrified': 2,  'caring':      3,  'disappointed': 4, \n","    'faithful': 5,  'joyful':       6,  'jealous':   7,  'disgusted':   8,  'surprised':    9, \n","    'ashamed':  10, 'afraid':       11, 'impressed': 12, 'sentimental': 13, 'devastated':   14, \n","    'excited':  15, 'anticipating': 16, 'annoyed':   17, 'anxious':     18, 'furious':      19, \n","    'content':  20, 'lonely':       21, 'angry':     22, 'confident':   23, 'apprehensive': 24, \n","    'guilty':   25, 'embarrassed':  26, 'grateful':  27, 'hopeful':     28, 'proud':        29, \n","    'prepared': 30, 'nostalgic':    31\n","}\n","\n","id_sent = {v: k for k, v in sent_id.items()}"]},{"cell_type":"markdown","metadata":{"id":"JY01-xchq_9u","outputId":"6681c51d-67ed-4c28-cfa9-8610de82bb3f"},"source":["## Build Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDFHMLxU38Ar","scrolled":true},"outputs":[],"source":["from transformers import PreTrainedTokenizerBase, AutoTokenizer\n","from transforms import (\n","    Tokenization,\n","    Encoding,\n","    RandomDeletion,\n","    RandomSwap,\n","    RandomMask,\n",")\n","\n","class EnsembledDataset(torch.utils.data.Dataset):\n","    def __init__(self, df, distilbert_tokenizer, distilroberta_tokenizer, augmentation, maxlen: Optional[int] = None):\n","        self.size = len(df)\n","        self.df = df.copy()\n","        self.augmentation = augmentation\n","        self.encodings = [\n","            distilbert_tokenizer(df[['prompt', 'conv']].values.tolist(), add_special_tokens=True, max_length=maxlen, padding='max_length', truncation=True),\n","            distilroberta_tokenizer(df[['prompt', 'conv']].values.tolist(), add_special_tokens=True, max_length=maxlen, padding='max_length', truncation=True),\n","        ]\n","        \n","        self.labels = None\n","        if 'label' in df.columns:\n","            self.labels = df['label'].values.tolist()\n","\n","    def __getitem__(self, idx):\n","        rawdata = self.df.iloc[idx][['prompt', 'conv']].values.tolist()\n","        aug_enc = self.augmentation(rawdata)\n","\n","        item = {k: torch.tensor([enc[k][idx] for enc in self.encodings] + [aug_enc[k]]) for k in ['input_ids', 'attention_mask']}\n","\n","        if self.labels:\n","            item['labels'] = torch.tensor(self.labels[idx])\n","        \n","        return item\n","\n","    def __len__(self):\n","        return self.size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BNMzJFUlW7m-"},"outputs":[],"source":["distilbert_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n","distilroberta_tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n","\n","special_tokens_dict = {'additional_special_tokens': ['[SPEAKER_A]', '[SPEAKER_B]']}\n","distilbert_tokenizer.add_special_tokens(special_tokens_dict)\n","distilroberta_tokenizer.add_special_tokens(special_tokens_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EX8crtsc4q0H"},"outputs":[],"source":["encoding = torch.nn.Sequential(\n","    Tokenization(distilroberta_tokenizer),\n","    Encoding(distilroberta_tokenizer, max_length=512)\n",")\n","\n","augmentation = torch.nn.Sequential(\n","    Tokenization(distilroberta_tokenizer),\n","    RandomDeletion(distilroberta_tokenizer, rate=0.1),\n","    RandomSwap(distilroberta_tokenizer, n_swap=1),\n","    RandomMask(distilroberta_tokenizer, rate=0.1),\n","    Encoding(distilroberta_tokenizer, max_length=512)\n",")\n","\n","train_dataset = EnsembledDataset(traindf, distilbert_tokenizer, distilroberta_tokenizer, augmentation, maxlen=512)\n","valid_dataset = EnsembledDataset(validdf, distilbert_tokenizer, distilroberta_tokenizer, encoding, maxlen=512)\n","test_dataset = EnsembledDataset(testdf, distilbert_tokenizer, distilroberta_tokenizer, encoding, maxlen=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XGAa3P_yHAtw"},"outputs":[],"source":["from datasets import load_metric\n","\n","metric_precision = load_metric('precision')\n","metric_recall = load_metric('recall')\n","metric_f1 = load_metric('f1')\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    precision = metric_precision.compute(predictions=predictions, references=labels, average='macro')['precision']\n","    recall = metric_recall.compute(predictions=predictions, references=labels, average='macro')['recall']\n","    f1_score = metric_f1.compute(predictions=predictions, references=labels, average='macro')['f1']\n","    return {'Precision': precision, 'Recall': recall, 'F1': f1_score}"]},{"cell_type":"markdown","metadata":{"id":"auZZvYFbQlbf"},"source":["## Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvUeBqRiMv9T"},"outputs":[],"source":["from torch.nn import CrossEntropyLoss\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","class StackedEnsembleModel(torch.nn.Module):\n","    def __init__(self, base_models: List, n_labels: int):\n","        super(StackedEnsembleModel, self).__init__()\n","        self.n_models = len(base_models)\n","        self.n_labels = n_labels\n","\n","        self.n_hidden_state = self.n_models * self.n_labels\n","\n","        self.base_models = base_models\n","        self.classifier = torch.nn.Sequential(OrderedDict([\n","                            ('pre-classifier', torch.nn.Linear(in_features=self.n_hidden_state, out_features=self.n_hidden_state, bias=True)),\n","                            ('dropout', torch.nn.Dropout(p=0.2)),\n","                            ('classifier', torch.nn.Linear(in_features=self.n_hidden_state, out_features=n_labels, bias=True))]))\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = torch.hstack([self.base_models[i](input_ids=input_ids[:, i], attention_mask=attention_mask[:, i]).logits for i in range(self.n_models)])\n","        outputs = self.classifier(outputs)\n","        loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            loss = loss_fct(outputs.view(-1, self.n_labels), labels.view(-1))\n","\n","        return SequenceClassifierOutput(loss=loss, logits=outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGFDh07iQyVV"},"outputs":[],"source":["# load fine-tuned model\n","from transformers import AutoModelForSequenceClassification, AutoConfig\n","\n","base_models = [AutoModelForSequenceClassification.from_pretrained(ckpt_path) for ckpt_path in BASE_MODEL_CKPTS]\n","for model in base_models:\n","    for p in model.parameters():\n","        p.requires_grad = False\n","    model.to(torch.device('cuda'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pzd9ATsZl5Mh"},"outputs":[],"source":["model = StackedEnsembleModel(base_models, n_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZ9zr4wx9rFv"},"outputs":[],"source":["# model.load_state_dict(torch.load(os.path.join(MODEL_SAVE_DIR, CHECKPOINT, 'pytorch_model.bin')))"]},{"cell_type":"markdown","metadata":{"id":"t0HAE--ARG_g"},"source":["## Build Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qh3UGZ2G38Au"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(\n","    output_dir=MODEL_SAVE_DIR,\n","    logging_strategy='epoch',\n","    evaluation_strategy='epoch',\n","    save_strategy='epoch',\n","    num_train_epochs=EPOCHS,\n","    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n","    per_device_eval_batch_size=VALID_BATCH_SIZE,\n","    lr_scheduler_type='cosine',\n","    learning_rate=LEARNING_RATE,\n","    warmup_steps=WARMUP,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"F1\",\n","    seed=SEED,\n","    data_seed=SEED\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=valid_dataset,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"markdown","metadata":{"id":"iYbLS28sBkUb"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOP7e99o38Av","scrolled":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"QLOg3ouKGka4"},"source":["## Prediction & Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlnXX0-15fTU"},"outputs":[],"source":["def post_processing(logits, threshold: float = 1., steps: int = 0):\n","    \"\"\"Replace the top1 prediction with other potential answers\n","\n","    Args:\n","        logits (Union[List, np.array]): the output hypothesis of the model\n","        threshold (float): top2 > top1 * threshold then top2 will be the result\n","        steps (int): how many candidates should be test\n","    \"\"\"\n","    n_data, n_classes = logits.shape\n","    logits = torch.softmax(torch.tensor(logits), dim=-1)\n","    top5_indices = torch.argsort(logits, dim=-1, descending=True)[:, :5] # top 5 label predictions\n","    result = top5_indices[:, 0].clone() # label predictions\n","\n","    # default result is just argmax, no candidates will be checked\n","    if threshold == 1. and steps == 0:\n","        return result\n","    \n","    # check if the second ans satisfies the threshold\n","    for i in range(n_data):\n","        if logits[i, top5_indices[i, 1]] > logits[i, top5_indices[i, 0]] * threshold:\n","            result[i] = top5_indices[i, 1]\n","\n","    # TODO: check the absent labels\n","\n","    return result\n","\n","def evaluate_f1(preds, labels, average='macro'):\n","    precision = metric_precision.compute(predictions=preds, references=labels, average=average)['precision']\n","    recall = metric_recall.compute(predictions=preds, references=labels, average=average)['recall']\n","    f1_score = metric_f1.compute(predictions=preds, references=labels, average=average)['f1']\n","    return {'Precision': precision, 'Recall': recall, 'F1': f1_score}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaeBbddhyq9O"},"outputs":[],"source":["test_preds = trainer.predict(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_iOG9b12uwP"},"outputs":[],"source":["test_ans = post_processing(test_preds.predictions)\n","testdf['pred'] = test_ans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeNRssvp6QNr"},"outputs":[],"source":["submission = pd.read_csv('data/fixed_test.csv')\n","submission['pred'] = np.zeros(shape=(len(submission),), dtype=int)\n","for _, row in testdf.iterrows():\n","    submission.loc[(submission['conv_id'] == row['conv_id']), 'pred'] = row['pred']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYeUX2p47Y80"},"outputs":[],"source":["submission"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDPpBxjI8p2Z"},"outputs":[],"source":["submission[['pred']].to_csv(f'output/20220609_ckpt{CHECKPOINT.split(\"-\")[-1]}_submission.csv', encoding='utf8')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["QLOg3ouKGka4"],"name":"Master.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.11"}},"nbformat":4,"nbformat_minor":0}